<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>Self-Attention Mechanism</title>
</head>
<body>
    <h2>Self-Attention Mechanism</h2>

    <p>This document provides an overview of the self-attention mechanism, which is fundamental in transformer models for tasks like natural language processing and computer vision.</p>

    <h3>Practical Implementation</h3>
    <ul>
        <li>The self-attention mechanism allows the model to weigh the importance of different parts of the input sequence dynamically. This ability to assign varying levels of importance is key to capturing long-range dependencies, which is highly effective in tasks like language translation, text summarization, and machine vision.</li>
        <li>The self-attention operation calculates attention scores for every input, determining how much focus to put on other inputs when generating a contextualized representation.</li>
    </ul>

    <h3>Mathematical Background</h3>
    <ul>
        <li><strong>Self-Attention Calculation</strong>:
            <br>
            Given an input sequence \(X\):
            \[
            Q = XW_Q, \quad K = XW_K, \quad V = XW_V
            \]
            Where \(Q\), \(K\), and \(V\) represent the Query, Key, and Value matrices respectively, and \(W_Q\), \(W_K\), \(W_V\) are learned weight matrices.
            <br>
            The attention score is computed as:
            \[
            \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
            \]
            Where \(d_k\) is the dimension of the key vectors.
        </li>
    </ul>
</body>
</html>
