<h2>Understanding Precision in Classification</h2>

<p>Precision is a key metric used in the evaluation of classification models, particularly in binary classification. It provides insight into the accuracy of the positive predictions made by the model.</p>

<h3>Mathematical Definition</h3>

<p>Precision is defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP):</p>

\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\]

<p>Where:</p>

<ul>
    <li><strong>True Positives (TP):</strong> The number of positive samples that are correctly identified as positive.</li>
    <li><strong>False Positives (FP):</strong> The number of negative samples that are incorrectly identified as positive.</li>
</ul>

<h3>Characteristics of Precision</h3>

<ul>
    <li><strong>Range:</strong> Precision ranges from 0 to 1, where 1 indicates perfect precision (no false positives) and 0 indicates no true positives.</li>
    <li><strong>Interpretation:</strong> High precision means that the model has a low false positive rate, meaning it rarely labels negative samples as positive.</li>
    <li><strong>Use Case:</strong> Precision is particularly useful when the cost of false positives is high, such as in medical diagnosis or fraud detection.</li>
</ul>

<p>In this problem, you will implement a function to calculate precision given the true labels and predicted labels of a binary classification task.</p>
